---
title: "Modeling R Notebook"
output: pdf_document
---

```{r include=FALSE} 
knitr::opts_chunk$set(message = FALSE, warning = FALSE) 
```

```{r}

library(caret)
library(rpart)
suppressMessages(library(tidyverse))
library(rpart.plot)
library(Metrics)
library(ipred)
library(randomForest)


shredCVL <- readRDS("capstone dataset/shredCVL")
table(shredCVL$bot)

#Outcome variable is 'bot' but it should be a factor
shredCVL$bot <- as.factor(ifelse(shredCVL$bot==1,'Yes','No'))

#Let's consider the important variables only
shredCVL <- shredCVL %>% select(statusesCount,friendsCount,followersCount,listedCount,
                                acct_age,langDiv,mean_time_betwn_tweets,mCount,bot)

#Always set seed for reproduciblity
set.seed(1234)
in_train <- createDataPartition(y = shredCVL$bot, p = .75, list = FALSE)
testing_set <- shredCVL[-in_train,]
training_set <- shredCVL[in_train,]

str(training_set)
table(testing_set$bot) #526,127
table(training_set$bot) #1581, 384

# both ratios hold to an 20:80 proportion.

model1 <- rpart(formula = bot ~., data = shredCVL, method = "class")

#Display the results
rpart.plot(x = model1, yesno = T, type = 2, extra = 2)

# Train the model (to predict 'bot')
bot_model <- rpart(formula = bot~., 
                      data = training_set, 
                      method = "class")

# Look at the model output                      
print(bot_model)

# Generate predicted classes using the model object
class_prediction <- predict(object = bot_model,  
                            newdata = testing_set,   
                            type = "class")  

# Calculate the confusion matrix for the test set
confusionMatrix(data = class_prediction,       
                reference = testing_set$bot)  

#Minimize impurity measure - Gini Index
#Lower the Gini Index, higher the purity of the split
# Train a gini-based model
bot_model1 <- rpart(formula = bot ~ ., 
                       data = training_set, 
                       method = "class",
                       parms = list(split = 'gini'))


# Train an information-based model
bot_model2 <- rpart(formula = bot ~ ., 
                       data = training_set, 
                       method = "class",
                       parms = list(split = 'information'))

# Generate predictions on the validation set using the gini model
pred1 <- predict(object = bot_model1, 
                 newdata = testing_set,
                 type = 'class')  

# Generate predictions on the validation set using the information model
pred2 <- predict(object = bot_model2, 
                 newdata = testing_set,
                 type = "class")

# Compare classification error
library(Metrics)
ce(actual = testing_set$bot, 
   predicted = pred1)
ce(actual = testing_set$bot, 
   predicted = pred2)  
dt_preds <- pred2

################################################
#Bagged Trees
# Train a bagged model
bot_model_bagged <- bagging(formula = bot ~ ., 
                        data = training_set,
                        coob = TRUE)

# Print the model
print(bot_model)

#Evaluate the performance
# Generate predicted classes using the model object
class_prediction <- predict(object = bot_model_bagged,    
                            newdata = testing_set,  
                            type = "class")  # return classification labels
# Print the predicted classes
table(class_prediction)

# Calculate the confusion matrix for the test set
confusionMatrix(data = class_prediction,       
                reference = testing_set$bot)  
# Generate predictions on the test set
pred <- predict(object = bot_model_bagged,
                newdata = testing_set,
                type = "prob")

# `pred` is a matrix
class(pred)

# Look at the pred format
head(pred)

# Compute the AUC (`actual` must be a binary (or 1/0 numeric) vector)
auc(actual = ifelse(testing_set$bot == "Yes", 1, 0), 
    predicted = pred[,"Yes"])

#Caret for cross validation
#train() and trainControl()
# Specify the training configuration
ctrl <- trainControl(method = "cv",     # Cross-validation
                     number = 5,      # 5 folds
                     classProbs = TRUE,                  # For AUC
                     summaryFunction = twoClassSummary)  # For AUC

# Cross validate the credit model using "treebag" method; 
# Track AUC (Area under the ROC curve)
set.seed(1)  # for reproducibility
credit_caret_model <- train(bot ~ .,
                            data = training_set, 
                            method = "treebag",
                            metric = "ROC",
                            trControl = ctrl,
                            na.action = na.exclude)

# Look at the model object
print(credit_caret_model)

# Inspect the contents of the model list 
names(credit_caret_model)

# Print the CV AUC
credit_caret_model$results[,"ROC"]
# Generate predictions on the test set
pred <- predict(object = credit_caret_model, 
                newdata = testing_set,
                type = "prob")


# Compute the AUC (`actual` must be a binary (or 1/0 numeric) vector)
auc(actual = ifelse(testing_set$bot == "Yes", 1, 0), 
    predicted = pred[,"Yes"])
# Print ipred::bagging test set AUC estimate
# print(credit_ipred_model_test_auc)

# Print caret "treebag" test set AUC estimate
# print(credit_caret_model_test_auc)

# Compare to caret 5-fold cross-validated AUC
credit_caret_model$results[, 2]
bag_preds <- pred


######################################################
#Random Forest
library(randomForest)
# Train a Random Forest
set.seed(1)  # for reproducibility
bot_model <- randomForest(formula = bot ~ ., 
                             data = training_set,
                          na.action = na.exclude)

# Print the model output                             
print(bot_model)
#mtry - the number of variables tried at each split (sqrt of no.of variables)
#OOB error rate is the error rate for the samples that were not selected
# Grab OOB error matrix & take a look
err <- bot_model$err.rate
head(err)

# Look at final OOB error rate (last row in err matrix)
oob_err <- err[nrow(err), "OOB"]
print(oob_err)

# Plot the model trained in the previous exercise
plot(bot_model)

# Add a legend since it doesn't have one by default
legend(x = "right", 
       legend = colnames(err),
       fill = 1:ncol(err))
# Generate predicted classes using the model object
class_prediction <- predict(object = bot_model,   # model object 
                            newdata = testing_set,  # test dataset
                            type = "class") # return classification labels

# Calculate the confusion matrix for the test set
cm <- confusionMatrix(data = class_prediction,       # predicted classes
                      reference = testing_set$bot)  # actual classes
print(cm)

# Compare test set accuracy to OOB accuracy
paste0("Test Accuracy: ", cm$overall[1])
paste0("OOB Accuracy: ", 1 - oob_err)

#OOB error vs Test Set error
# Generate predictions on the test set
pred <- predict(object = bot_model,
                newdata = testing_set,
                type = "prob")

# `pred` is a matrix
class(pred)

# Look at the pred format
head(pred)

# Compute the AUC (`actual` must be a binary 1/0 numeric vector)
auc(actual = ifelse(testing_set$bot == "Yes", 1, 0), 
    predicted = pred[,"Yes"])  
```
