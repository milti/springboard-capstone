---
title: "shredCVL ML Notebook"
output:
  pdf_document: default
---

Using the template of the machine learning module in datacamp.


```{r global_options, include=FALSE} 
knitr::opts_chunk$set(fig.width=12, fig.height=8, fig.path='Figs/', echo=FALSE, warning=FALSE, message=FALSE)

```

```{r, message = FALSE}

library(ranger)
library(e1071)
library(caret)
library(ggplot2)  
library(lattice)
#library(RBackend)
library(mlbench)
library(caTools)
library(C50)
library(caretEnsemble)
library(rpart)
(library("tidyverse"))
library(rpart.plot)

```

```{r, message = FALSE, warning = FALSE}

shredCVL <- readRDS("capstone dataset/shredCVL")

shredCVL <- na.omit(shredCVL)
table(shredCVL$bot)

shredCVL$bot <- as.factor(shredCVL$bot)
shredCVL <- shredCVL[,c(3:6,9:11,15,2)]

in_train <- createDataPartition(y = shredCVL$bot, p = .75, list = FALSE)
testing_set <- shredCVL[-in_train,]
training_set <- shredCVL[in_train,]

str(training_set)
table(testing_set$bot)
table(training_set$bot)


# Fit lm modell: model
model <- lm(bot ~ ., shredCVL)

# Predict on full data: p
p <- predict(model, shredCVL)

# Compute errors: error
error <- p - as.integer(shredCVL$bot)

# Calculate RMSE
sqrt(mean(error^2, na.rm = TRUE))

# Set seed
set.seed(2324)

# Shuffle row indices: rows
rows <- sample(nrow(shredCVL))

# Randomly order data
shredCVL <- shredCVL[rows, ]

# Determine row to split on: split
split <- round(nrow(shredCVL) * .80)

# Create train
train <- shredCVL[1:split, ]

# Create test
test <- shredCVL[(split + 1):nrow(shredCVL), ]

# Fit lm model2 on train: modell
model2 <- lm(bot ~ ., train)

# Predict on test: p
p2 <- predict(model2, test)

colAUC(p2, test$bot, plotROC = TRUE)

# Compute errors: error
error2 <- p2 - as.integer(test$bot)

# Calculate RMSE
sqrt(mean(error2^2, na.rm = TRUE))
```

```{r}
# Fit lm model3 using 10-fold CV: model3
model3 <- train(
  bot ~ ., shredCVL,
  method = "glm",
  trControl = trainControl(
    method = "cv", number = 10,
    verboseIter = TRUE
  )
)

# Print model3 to console
print(model3)

# Fit lm model4 using 5-fold CV: model4
model4 <- train(
  bot ~ ., shredCVL,
  method = "glm",
  trControl = trainControl(
    method = "cv", number = 5,
    verboseIter = TRUE
  )
)

# Print model4 to console
model4

# Fit lm modell using 5 x 5-fold CV: model5
model5 <- train(
  bot ~ ., shredCVL,
  method = "glm",
  trControl = trainControl(
    method = "cv", number = 5,
    repeats = 5, verboseIter = TRUE
  )
)

# Print model5 to console
model5

# Predict on full Boston dataset
predict(model5, shredCVL)
```

```{r}
# Shuffle row indices: rows
rows <- sample(nrow(shredCVL))

# Randomly order data: Sonar
shredCVL <- shredCVL[rows, ]

# Identify row to split on: split
split <- round(nrow(shredCVL) * .60)

# Create train
train2 <- shredCVL[1:split, ]

# Create test
test2 <- shredCVL[(split + 1):nrow(shredCVL), ]

# Fit glm modell: model6
model6 <- glm(bot ~ ., family = "binomial", train)

# Predict on test: p
p3 <- predict(model6, test, type = "response")

colAUC(p3, test$bot, plotROC = TRUE)

# Calculate class probabilities: p_class
p_class3 <- ifelse(p3 > 0.75, "1", "0")

# Create confusion matrix
confusionMatrix(p_class3, test[["bot"]])

# Apply threshold of 0.9: p_class
p_class4 <- ifelse(p3 > .9, "1", "0")

# Create confusion matrix
confusionMatrix(p_class3, test[["bot"]])

# Apply threshold of 0.10: p_class
p_class5 <- ifelse(p3 > .1, "1", "0")

# Create confusion matrix
confusionMatrix(p_class3, test[["bot"]])

# Predict on test: p4
p4 <-  predict(model6, test, type = "response")

# Make ROC curve
colAUC(p4, test$bot, plotROC = TRUE)

```



```{r}
# Create trainControl object: myControl
myControl <- trainControl(
  method = "cv", 
  number = 10,
  summaryFunction = twoClassSummary,
  classProbs = TRUE, # IMPORTANT!
  verboseIter = TRUE
)

# Train glm with custom trainControl: model7
model7 <- train(make.names(bot) ~ ., shredCVL, method = "glm", trControl = myControl, na.action = na.pass)

# Print model7 to console
model7

```



```{r}

#levels(shredCVL[,9])<-c("no", "yes")


# Fit random forest: model8
model8 <- train(
  bot ~ .,
  tuneLength = 1,
  data = shredCVL, method = "ranger",
  trControl = trainControl(method = "cv", number = 5, verboseIter = TRUE),
  na.action = na.pass
)

# Print model8 to console
model8

# Fit random forest: model9
model9 <- train(
  bot ~ .,
  tuneLength = 3,
  data = shredCVL, method = "ranger",
  trControl = trainControl(method = "cv", number = 5, verboseIter = TRUE),
  na.action = na.pass
)

# Print modell to console
model9

# Plot modell
plot(model9)

# Fit random forest: model10
#model10 <- train(
#  quality ~ .,
#  tuneGrid = data.frame(mtry = c(2, 3, 7)),
#  data = wine, method = "ranger",
#  trControl = trainControl(method = "cv", number = 5, verboseIter = TRUE)
#)

# Print model10 to console
#model10

# Plot model10
#plot(model10)

```


```{r}
# Create custom trainControl: myControl
myControl <- trainControl(
  method = "cv", number = 10,
  summaryFunction = twoClassSummary,
  classProbs = TRUE, # IMPORTANT!
  verboseIter = TRUE
)

levels(shredCVL[,9])<-c("no", "yes")

# Fit glmnet model: model
model11 <- train(
  bot ~ ., shredCVL,
  method = "glmnet",
  trControl = myControl,
  na.action = na.pass
)

# Print model to console
model11

# Print maximum ROC statistic
max(model11[["results"]])

# Train glmnet with custom trainControl and tuning: model12
model12 <- train(
  bot ~ ., shredCVL,
  tuneGrid = expand.grid(alpha = 0:1,
                         lambda = seq(0.0001, 1, length = 20)),
  method = "glmnet",
  trControl = myControl,
  na.action = na.pass
)

# Print model12 to console
model12

# Print maximum ROC statistic
max(model12[["results"]][["ROC"]])

```


```{r}

levels(shredCVL[,9])<-c("no", "yes")

# Apply median imputation: model13
model13 <- train(
  x = shredCVL[,-9], y = shredCVL[,9],
  method = "glm",
  trControl = myControl,
  preProcess = "medianImpute"
)

# Print model13 to console
model13

# Apply KNN imputation: model14
model14 <- train(
  x = shredCVL[,-9], y = shredCVL[,9],
  method = "glm",
  trControl = myControl,
  preProcess = "knnImpute"
)

# Print model14 to console
model14

# Fit glm with median imputation: model15
model15 <- train(
  x = shredCVL[,-9], y = shredCVL[,9],
  method = "glm",
  trControl = myControl,
  preProcess = "medianImpute"
)

# Print model15
model15

# Fit glm with median imputation and standardization: model16
model16 <- train(
  x = shredCVL[,-9], y = shredCVL[,9],
  method = "glm",
  trControl = myControl,
  preProcess = c("medianImpute", "center", "scale")
)

# Print model16
model16

```


```{r, message=FALSE}
# Identify near zero variance predictors: remove_cols
remove_cols <- nearZeroVar(shredCVL, names = TRUE, 
                           freqCut = 2, uniqueCut = 20)

# Get all column names from bloodbrain_x: all_cols
all_cols <- names(shredCVL)

# Remove from data: bloodbrain_x_small
shredCVL_small <- shredCVL[ , setdiff(all_cols, remove_cols)]

# Fit model on reduced data: model17
model17 <- train(x = shredCVL_small, y = shredCVL$bot, method = "glm")

# Print modell to console
model17

# Fit modell on reduced data: model18
model18 <- train(x = shredCVL_small, y = shredCVL$bot, method = "glm")

# Print modell to console
model18

```


```{r}

# Create custom indices: myFolds
myFolds <- createFolds(shredCVL$bot, k = 5)

# Create reusable trainControl object: myControl
myControl <- trainControl(
  summaryFunction = twoClassSummary,
  classProbs = TRUE, # IMPORTANT!
  verboseIter = TRUE,
  savePredictions = TRUE,
  index = myFolds
)


# Fit glmnet modell: model_glmnet
model_glmnet <- train(
  x = shredCVL[,-9], y = shredCVL$bot,
  metric = "ROC", 
  method = "glmnet",
  trControl = myControl
)

# Fit random forest: model_rf
model_rf <- train(
  x = shredCVL[,-9], y = shredCVL$bot,
  metric = "ROC",
  method = "ranger",
  trControl = myControl
)
# Create model_list
model_list <- list(item1 = model_glmnet, item2 = model_rf)

# Pass modell_list to resamples(): resamples
resamples <- resamples(model_list)

# Summarize the results
summary(resamples)

# Create bwplot
bwplot(resamples, metric = "ROC")

# Create xyplot
xyplot(resamples, metric = "ROC")

# Create ensemble modell: stack
#stack <- caretStack(model_list, method = "glm")

# Look at summary
#summary(stack)
```