---
title: "springboard capstone"
author: "milti leonard"
date: "7/9/2017"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Detecting Bot Communities on Twitter and Determining Their Influence

Social media commands a tremendous influence on the ongoing zeitgeist. This influence can have IRL (or real world) consequences and repercussions. The intent of the platform is to allow people of various backgrounds, but similar interests, to connect and form communities of the like-minded that is not tied to geospatial constraints. However, the reality is that various agents create and unleash *bot* accounts that disrupt those communities and alters the digital landscape making it at times a dark and hostile place.

This project will focus on a selected set of *loaded* hashtags (meaning those whose LIWC is representative of a defined agenda) that command a respectable trending status to see whether or no it attracts these *bot* accounts and to what degree. Also to be investigated, is whether it is the human accounts or their *bot* counterparts that take the lead in establishing that trending status. I'm using as a roadmap Erin Shellman's class assignment on classifying whether a Twitter account is a bot or not, the problem statement (as well as a description of the dataset) can be found here, https://github.com/erinshellman/BI-TECH-CP303/blob/master/projects/project%202/problem_statement_project_2.md.

That project did not involve (nor describe) the gathering and ETL of the dataset presented to her students. The package I used to gather tweets to shape my dataset is as follows:

1. I leveraged another college project to first gather Twitter accounts denoted to be bots, *Debot*, found here, https://github.com/nchavoshi/debot_api. Using an API key I selected my *hashtag* and collected a set of tweets that had used the *hashtag* and were determined (by this program) to be *bot* accounts. This set included whether or no the *tweet* was a *retweet*, a *reply* or an original *tweet*. 

```{r, set-options, eval=FALSE, cache=FALSE}

library(reticulate)
library(XML)
#path_to_python <- "/usr/local/bin/python3.6"
path_to_python <- "/Library/Frameworks/Python.framework/Versions/3.6/bin/python3.6"
use_python(path_to_python)
dbot=import("debot")
db = dbot$DeBot('YocR3mKAc7U6hjKZrNnOCk2jjnWrqgSfwWYo8yOb')#This is my API key
cbots=htmlParse(db$get_related_bots('Charlottesville'))


#Now parse the XML data into a dataframe
botlist <- xmlToList(cbots)

library(listviewer)
flatbot <- flatten(botlist)
flatterbot <- flatten(flatbot)
remove(flatbot)
flatbot <- flatten(flatterbot)
newBotlist <- unname(sapply(flatbot, `[`, 2))

```

2. Utilising the _twitteR_ package, I used the extracted account names from the group of tweets gathered from *Debot* step and gathered information on the accounts themselves, such as when the account was created and the number of tweets thus far.


```{r, set-options, eval=FALSE, cache=FALSE}

library(twitteR)
library(ROAuth)

requestURL <- "https://api.twitter.com/oauth/request_token"
accessURL <- "https://api.twitter.com/oauth/access_token"
authURL <- "https://api.twitter.com/oauth/authorize"
requestURL <- "https://api.twitter.com/oauth/request_token"
accessURL <- "https://api.twitter.com/oauth/access_token"
authURL <- "https://api.twitter.com/oauth/authorize"
consumerKey <- "qkgJvUbFX70rNn1K4uq9NAuVs"
consumerSecret <- "xOGEeIMUUTrCTkVwGXXwdu75kqVzEq2yzDJuZPrBwt9NfwwFJm"
accessToken <- "1240280636-lHi3fdKVZGNE6FsUlvY7y13RsqED0JQgmKY9f74"
accessTokenSecret <- "jaqKjy4O6O73rYzOxMSAlbqigZu4iaEClAestfrRBjBZo"
handle <- "conjja"

setup_twitter_oauth(consumerKey, consumerSecret, accessToken, accessTokenSecret)

botAccts <- lookupUsers(newBotlist)
## or just load the dataset: > botAccts <- load("capstone dataset/botAccts")

botAcctlist <- twListToDF(botAccts)
acctNames <- row.names.data.frame(botAcctlist)

```

3. I then took the same accounts and gathered 200 of the most recent tweets (or as many as I could retrieve) from those accounts to get enough messaging to test the account's linguistic diversity.


```{r, set-options, eval=FALSE, cache=FALSE}

library(stringr)

recentTweetsDS <- map(unlistedBots[], ~searchTwitteR(.x, resultType = "recent", n=200))
recentTweetsDF <- twListToDF(flatten(recentTweetsDS))

```

4. A new dataframe was created to contain the variables that we will consider interesting. Then in order to truly test the linguistic diversity of the account, it's important to separate the *tweets* from the *retweets*.

```{r, set-options, eval=FALSE, cache=FALSE}
botDS <- data.frame(botAcctlist$screenName, botAcctlist$id, botAcctlist$created, botAcctlist$statusesCount, langDiv = 0, mean_time_betwn_tweets = 0, bot = 0)
columnnames <- names(botDS)
columnnames[1] <- "screenName"
columnnames[2] <- "ID"
columnnames[3] <- "acct_created"
columnnames[4] <- "statusesCount"
colnames(botDS) <- columnnames

tweetsRT <- filter(recentTweetsDF, isRetweet == "TRUE")
tweetsOPEN <- filter(recentTweetsDF, isRetweet != "TRUE")
#tweeters <- unique(tweetsOPEN$screenName)
#length(tweeters)

groupedTweetsDF <- tweetsOPEN %>%
  group_by(screenName) %>%
  filter(screenName %in% botDS$screenName)
castoffDF <- setdiff(tweetsOPEN, groupedTweetsDF)

```

The R packages that will be utilised in this study are as follows (with one or two being dropped, dependent on subsequent decisions made in the experimental design)

5. In this step, the linguistic diversity of the account is tested by taking the original *tweets* and averaging the 

```{r, set-options, eval=FALSE, cache=FALSE}
library(quanteda)

#using quanteda pkg
langDiv <- textstat_lexdiv(dfm(groupedTweetsDF$text))

botDiv <- groupedTweetsDF %>%
  group_by(screenName)
  summarise(meanDiv = mean(langDiv))

botDS <- left_join(botDS, botDiv)
botDS$langDiv <- botDS$meanDiv
#remove the meanDiv column for cleanup

```

```{r, set-options, eval=FALSE, cache=FALSE}

library(rtweet)

CTV <- search_tweets(
"#Charlottesville", n = 18000, include_rts = FALSE
)
ts_plot(CTV)

```

```{r, eval=FALSE}
#getting the source of the tweet (phone, web, other)
uniqSource <- unique(recentTweetsDF$statusSource)
inD <- which(str_detect(recentTweetsDF$statusSource, "Android"))
androidTweetsDF <- recentTweetsDF[inD,]

inD <- which(str_detect(recentTweetsDF$statusSource, "iP"))
iTweetsDF <- recentTweetsDF[inD,]

inD <- which(str_detect(recentTweetsDF$statusSource, "Web"))
webTweetsDF <- recentTweetsDF[inD,]

inD <- which(str_detect(recentTweetsDF$statusSource, paste(c("Android", "iP", "Web"), collapse = '|')))
otherTweetsDF <- recentTweetsDF[-inD,]

```

