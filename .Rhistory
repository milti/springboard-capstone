newdata = testing_set,
type = "class")
# Display the results
rpart.plot(x = model2, yesno = 2, type = 0, extra = 0)
######################
# Total number of rows in the credit data frame
#credit <- creditsub
#n <- nrow(credit)
# Number of rows for the training set (80% of the dataset)
#n_train <- round(0.8 * n)
# Create a vector of indices which is an 80% random sample
#set.seed(123)
#train_indices <- sample(1:n, n_train)
# Subset the credit data frame to training indices only
#training_set <- credit[train_indices, ]
# Exclude the training indices to create the test set
#testing_set <- credit[-train_indices, ]
# Train the model (to predict 'default')
model3 <- rpart(formula,
data = training_set,
method = "class", na.action = na.omit)
# Look at the model output
print(model3)
#Evaluate performance
#library(caret)
# Generate predicted classes using the model object
class_prediction <- predict(object = model3,
newdata = testing_set,
type = "class")
# Calculate the confusion matrix for the test set
#confusionMatrix(data = class_prediction,
#               reference = testing_set$default)
#Minimize impurity measure - Gini Index
#Lower the Gini Index, higher the purity of the split
# Train a gini-based model
model31 <- rpart(formula,
data = training_set,
method = "class",
parms = list(split = 'gini'))
# Train an information-based model
model32 <- rpart(formula,
data = training_set,
method = "class",
parms = list(split = 'information'))
# Generate predictions on the validation set using the gini model
pred31 <- predict(object = model31,
newdata = testing_set,
type = 'class')
# Generate predictions on the validation set using the information model
pred32 <- predict(object = model32,
newdata = testing_set,
type = "class")
# Display the results
rpart.plot(x = model31, yesno = 2, type = 0, extra = 0)
rpart.plot(x = model32, yesno = 2, type = 0, extra = 0)
######################
# Total number of rows in the credit data frame
#credit <- creditsub
#n <- nrow(credit)
# Number of rows for the training set (80% of the dataset)
#n_train <- round(0.8 * n)
# Create a vector of indices which is an 80% random sample
#set.seed(123)
#train_indices <- sample(1:n, n_train)
# Subset the credit data frame to training indices only
#training_set <- credit[train_indices, ]
# Exclude the training indices to create the test set
#testing_set <- credit[-train_indices, ]
# Train the model (to predict 'default')
modelClass <- rpart(formula,
data = training_set,
method = "class", na.action = na.omit)
# Look at the model output
print(modelClass)
#Evaluate performance
#library(caret)
# Generate predicted classes using the model object
class_prediction <- predict(object = modelClass,
newdata = testing_set,
type = "class")
# Calculate the confusion matrix for the test set
confusionMatrix(data = class_prediction,
reference = testing_set$bot)
#Minimize impurity measure - Gini Index
#Lower the Gini Index, higher the purity of the split
# Train a gini-based model
model41 <- rpart(formula,
data = training_set,
method = "class",
parms = list(split = 'gini'))
# Train an information-based model
model42 <- rpart(formula,
data = training_set,
method = "class",
parms = list(split = 'information'))
# Generate predictions on the validation set using the gini model
pred41 <- predict(object = model41,
newdata = testing_set,
type = 'class')
# Generate predictions on the validation set using the information model
pred42 <- predict(object = model42,
newdata = testing_set,
type = "class")
# Compare classification error
library(Metrics)
ce(actual = testing_set$bot,
predicted = pred1)
# Compare classification error
library(Metrics)
ce(actual = testing_set$bot,
predicted = pred41)
ce(actual = testing_set$bot,
predicted = pred42)
dt_preds <- pred2
# Compare classification error
library(Metrics)
ce(actual = testing_set$bot,
predicted = pred41)
ce(actual = testing_set$bot,
predicted = pred42)
dt_preds <- pred42
bagged_model = train(formula,
method = 'treebag',
data = training_set, na.action = na.omit)
print(bagged_model)
plot(varImp(bagged_model))
bagged_predictions = predict(bagged_model, testing_set)
#confusionMatrix(bagged_predictions, testing_set$bot)
boost_model = train(formula,
method = 'gbm',
data = training_set,
verbose = FALSE, na.action = na.omit)
print(boost_model)
plot(boost_model)
#plot(varImp(boost_model))
summary(boost_model$finalModel)
# predict
boost_predictions = predict(boost_model, testing_set)
confusionMatrix(boost_predictions, testing_set$bot)
shredCVL2 <- readRDS("capstone dataset/shredCVL")
table(shredCVL2$bot)
shredCVL <- readRDS("capstone dataset/shredCVL")
source('~/Documents/springboard-capstone/Modeling.R', echo=TRUE)
library(caret)
library(rpart)
library(tidyverse)
library(rpart.plot)
shredCVL <- readRDS("capstone dataset/shredCVL")
table(shredCVL$bot)
in_train <- createDataPartition(y = shredCVL$bot, p = .75, list = FALSE)
testing_set <- shredCVL[-in_train,]
training_set <- shredCVL[in_train,]
str(training_set)
table(testing_set$bot)
table(training_set$bot)
121/654
390/1964
# both ratios hold to an 20:80 proportion.
model1 <- rpart(formula = bot ~., data = shredCVL, method = "class")
str(model1)
print(model1)
formula <- bot ~ statusesCount + friendsCount + followersCount + listedCount + acct_age + langDiv + mean_time_betwn_tweets + Count + mCount
model2 <- rpart(formula, data = shredCVL, method = "class")
str(model2)
print(model2)
plotcp(model2)
rpart.plot(model2)
library(GGally)
ggpairs(training_set[,c(-1,-2,-7,-8,-12,-14)])
ggpairs(training_set[ , c('followersCount', 'friendsCount', 'acct_age',
'langDiv', 'statusesCount', 'bot')])
ggplot(filter(training_set, followersCount < 100),
aes(x = followersCount, fill = bot)) +
geom_histogram(binwidth = 3)
ggplot(filter(training_set, followersCount < 100),
aes(x = followersCount, fill = bot)) +
geom_histogram(binwidth = 3) +
facet_grid(bot ~.)
# how about the number of people they follow?
ggplot(training_set, aes(x = friendsCount, fill = bot)) +
geom_histogram(binwidth = 3000) +
facet_grid(bot ~.)
# Some people have a lot of followers, but most don't. we need to lob off
# the long tail so we can see the distribution better
ggplot(filter(training_set, followersCount < 100),
aes(x = followersCount, fill = bot)) +
geom_histogram()
ggplot(filter(training_set, followersCount < 100),
aes(x = followersCount, fill = bot)) +
geom_histogram() +
facet_grid(bot ~.)
# how about the number of people they follow?
ggplot(training_set, aes(x = friendsCount, fill = bot)) +
geom_histogram() +
facet_grid(bot ~.)
# it's a little hard to see
ggplot(filter(training_set, friendsCount < 2500),
aes(x = friendsCount, fill = bot)) +
geom_histogram() +
facet_grid(bot ~.)
# what about account age?
ggplot(training_set, aes(x = acct_age, fill = bot)) +
geom_histogram() +
facet_grid(bot ~.)
# lexical diversity
ggplot(training_set, aes(x = langDiv, fill = bot)) +
geom_histogram() +
facet_grid(bot ~.)
# what are the average values?
meanDiv =
training_set %>%
group_by(bot) %>%
summarize(meanDiv = mean(langDiv, na.rm = TRUE))
# add it to the plot
ggplot(training_set, aes(x = langDiv, fill = bot)) +
geom_histogram() +
geom_vline(data = meanDiv, aes(xintercept = meanDiv)) +
facet_grid(bot ~.)
bagged_model = train(formula,
method = 'treebag',
data = training_set, na.action = na.omit)
print(bagged_model)
plot(varImp(bagged_model))
bagged_predictions = predict(bagged_model, testing_set)
#confusionMatrix(bagged_predictions, testing_set$bot)
boost_model = train(formula,
method = 'gbm',
data = training_set,
verbose = FALSE, na.action = na.omit)
print(boost_model)
plot(boost_model)
#plot(varImp(boost_model))
summary(boost_model$finalModel)
# predict
boost_predictions = predict(boost_model, testing_set)
confusionMatrix(boost_predictions, testing_set$bot)
install.packages(c("brglm2", "dwapi", "MASS", "rpart", "usethis"))
suppressMessages(library("tidyverse"))
library(caret)
library(rpart)
sessionInfo()
detach(ipred)
detach(rpart)
detach("ipred")
sessionInfo()
knitr::opts_chunk$set(messages = FALSE)
library(rpart)
library(caret)
library(tidyverse)
library(rpart.plot)
shredCVL <- readRDS("capstone dataset/shredCVL")
library(rpart)
library(caret)
suppressMessages(library("tidyverse"))
library(rpart.plot)
shredCVL <- readRDS("capstone dataset/shredCVL")
table(shredCVL$bot)
in_train <- createDataPartition(y = shredCVL$bot, p = .75, list = FALSE)
testing_set <- shredCVL[-in_train,]
training_set <- shredCVL[in_train,]
str(training_set)
table(testing_set$bot)
table(training_set$bot)
121/654
390/1964
# both ratios hold to an 20:80 proportion.
model1 <- rpart(formula = bot ~., data = shredCVL, method = "class")
str(model1)
print(model1)
formula <- bot ~ statusesCount + friendsCount + followersCount + listedCount + acct_age + langDiv + mean_time_betwn_tweets + mCount
model2 <- rpart(formula, data = shredCVL, method = "class")
str(model2)
print(model2)
plotcp(model2)
rpart.plot(model2)
library(GGally)
ggpairs(training_set[,c(-1,-2,-7,-8,-12,-14)])
ggpairs(training_set[ , c('followersCount', 'friendsCount', 'acct_age',
'langDiv', 'statusesCount', 'bot')])
ggplot(filter(training_set, followersCount < 100),
aes(x = followersCount, fill = bot)) +
geom_histogram(binwidth = 3)
ggplot(filter(training_set, followersCount < 100),
aes(x = followersCount, fill = bot)) +
geom_histogram(binwidth = 3) +
facet_grid(bot ~.)
# how about the number of people they follow?
ggplot(training_set, aes(x = friendsCount, fill = bot)) +
geom_histogram(binwidth = 3000) +
facet_grid(bot ~.)
# Some people have a lot of followers, but most don't. we need to lob off
# the long tail so we can see the distribution better
ggplot(filter(training_set, followersCount < 100),
aes(x = followersCount, fill = bot)) +
geom_histogram()
ggplot(filter(training_set, followersCount < 100),
aes(x = followersCount, fill = bot)) +
geom_histogram() +
facet_grid(bot ~.)
# how about the number of people they follow?
ggplot(training_set, aes(x = friendsCount, fill = bot)) +
geom_histogram() +
facet_grid(bot ~.)
# it's a little hard to see
ggplot(filter(training_set, friendsCount < 2500),
aes(x = friendsCount, fill = bot)) +
geom_histogram() +
facet_grid(bot ~.)
# what about account age?
ggplot(training_set, aes(x = acct_age, fill = bot)) +
geom_histogram() +
facet_grid(bot ~.)
# lexical diversity
ggplot(training_set, aes(x = langDiv, fill = bot)) +
geom_histogram() +
facet_grid(bot ~.)
# what are the average values?
meanDiv =
training_set %>%
group_by(bot) %>%
summarize(meanDiv = mean(langDiv, na.rm = TRUE))
# add it to the plot
ggplot(training_set, aes(x = langDiv, fill = bot)) +
geom_histogram() +
geom_vline(data = meanDiv, aes(xintercept = meanDiv)) +
facet_grid(bot ~.)
head(str(model1))
glimpse(model1)
options(width = 80)
glimpse(model1)
library(caret)
library(rpart)
library(tidyverse)
library(rpart.plot)
library(Metrics)
library(ipred)
library(randomForest)
shredCVL <- readRDS("capstone dataset/shredCVL")
table(shredCVL$bot)
#Outcome variable is 'bot' but it should be a factor
shredCVL$bot <- as.factor(ifelse(shredCVL$bot==1,'Yes','No'))
#Let's consider the important variables only
shredCVL <- shredCVL %>% select(statusesCount,friendsCount,followersCount,listedCount,
acct_age,langDiv,mean_time_betwn_tweets,mCount,bot)
#Always set seed for reproduciblity
set.seed(1234)
in_train <- createDataPartition(y = shredCVL$bot, p = .75, list = FALSE)
testing_set <- shredCVL[-in_train,]
training_set <- shredCVL[in_train,]
str(training_set)
table(testing_set$bot) #526,127
table(training_set$bot) #1581, 384
# both ratios hold to an 20:80 proportion.
model1 <- rpart(formula = bot ~., data = shredCVL, method = "class")
#Display the results
rpart.plot(x = model1, yesno = T, type = 2, extra = 2)
# Train the model (to predict 'bot')
bot_model <- rpart(formula = bot~.,
data = training_set,
method = "class")
# Look at the model output
print(bot_model)
# Generate predicted classes using the model object
class_prediction <- predict(object = bot_model,
newdata = testing_set,
type = "class")
# Calculate the confusion matrix for the test set
confusionMatrix(data = class_prediction,
reference = testing_set$bot)
#Minimize impurity measure - Gini Index
#Lower the Gini Index, higher the purity of the split
# Train a gini-based model
bot_model1 <- rpart(formula = bot ~ .,
data = training_set,
method = "class",
parms = list(split = 'gini'))
# Train an information-based model
bot_model2 <- rpart(formula = bot ~ .,
data = training_set,
method = "class",
parms = list(split = 'information'))
# Generate predictions on the validation set using the gini model
pred1 <- predict(object = bot_model1,
newdata = testing_set,
type = 'class')
# Generate predictions on the validation set using the information model
pred2 <- predict(object = bot_model2,
newdata = testing_set,
type = "class")
# Compare classification error
library(Metrics)
ce(actual = testing_set$bot,
predicted = pred1)
ce(actual = testing_set$bot,
predicted = pred2)
dt_preds <- pred2
################################################
#Bagged Trees
# Train a bagged model
bot_model_bagged <- bagging(formula = bot ~ .,
data = training_set,
coob = TRUE)
# Print the model
print(bot_model)
#Evaluate the performance
# Generate predicted classes using the model object
class_prediction <- predict(object = bot_model_bagged,
newdata = testing_set,
type = "class")  # return classification labels
# Print the predicted classes
print(class_prediction)
# Calculate the confusion matrix for the test set
confusionMatrix(data = class_prediction,
reference = testing_set$bot)
# Generate predictions on the test set
pred <- predict(object = bot_model_bagged,
newdata = testing_set,
type = "prob")
# `pred` is a matrix
class(pred)
# Look at the pred format
head(pred)
# Compute the AUC (`actual` must be a binary (or 1/0 numeric) vector)
auc(actual = ifelse(testing_set$bot == "Yes", 1, 0),
predicted = pred[,"Yes"])
#Caret for cross validation
#train() and trainControl()
# Specify the training configuration
ctrl <- trainControl(method = "cv",     # Cross-validation
number = 5,      # 5 folds
classProbs = TRUE,                  # For AUC
summaryFunction = twoClassSummary)  # For AUC
# Cross validate the credit model using "treebag" method;
# Track AUC (Area under the ROC curve)
set.seed(1)  # for reproducibility
credit_caret_model <- train(bot ~ .,
data = training_set,
method = "treebag",
metric = "ROC",
trControl = ctrl,
na.action = na.exclude)
# Look at the model object
print(credit_caret_model)
# Inspect the contents of the model list
names(credit_caret_model)
# Print the CV AUC
credit_caret_model$results[,"ROC"]
# Generate predictions on the test set
pred <- predict(object = credit_caret_model,
newdata = testing_set,
type = "prob")
# Compute the AUC (`actual` must be a binary (or 1/0 numeric) vector)
auc(actual = ifelse(testing_set$bot == "Yes", 1, 0),
predicted = pred[,"Yes"])
# Print ipred::bagging test set AUC estimate
# print(credit_ipred_model_test_auc)
# Print caret "treebag" test set AUC estimate
# print(credit_caret_model_test_auc)
# Compare to caret 5-fold cross-validated AUC
credit_caret_model$results[, 2]
bag_preds <- pred
######################################################
#Random Forest
library(randomForest)
# Train a Random Forest
set.seed(1)  # for reproducibility
bot_model <- randomForest(formula = bot ~ .,
data = training_set,
na.action = na.exclude)
# Print the model output
print(bot_model)
#mtry - the number of variables tried at each split (sqrt of no.of variables)
#OOB error rate is the error rate for the samples that were not selected
# Grab OOB error matrix & take a look
err <- bot_model$err.rate
head(err)
# Look at final OOB error rate (last row in err matrix)
oob_err <- err[nrow(err), "OOB"]
print(oob_err)
# Plot the model trained in the previous exercise
plot(bot_model)
# Add a legend since it doesn't have one by default
legend(x = "right",
legend = colnames(err),
fill = 1:ncol(err))
# Generate predicted classes using the model object
class_prediction <- predict(object = bot_model,   # model object
newdata = testing_set,  # test dataset
type = "class") # return classification labels
# Calculate the confusion matrix for the test set
cm <- confusionMatrix(data = class_prediction,       # predicted classes
reference = testing_set$bot)  # actual classes
print(cm)
# Compare test set accuracy to OOB accuracy
paste0("Test Accuracy: ", cm$overall[1])
paste0("OOB Accuracy: ", 1 - oob_err)
#OOB error vs Test Set error
# Generate predictions on the test set
pred <- predict(object = bot_model,
newdata = testing_set,
type = "prob")
# `pred` is a matrix
class(pred)
# Look at the pred format
head(pred)
# Compute the AUC (`actual` must be a binary 1/0 numeric vector)
auc(actual = ifelse(testing_set$bot == "Yes", 1, 0),
predicted = pred[,"Yes"])
##########################################################################
View(credit_caret_model)
print(credit_caret_model)
View(bag_preds)
