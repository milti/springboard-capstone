---
title: "springboard capstone"
author: "milti leonard"
date: "7/9/2017"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Detecting Bot Communities on Twitter and Determining Their Influence

Social media commands a tremendous influence on the ongoing zeitgeist. This influence can have IRL (or real world) consequences and repercussions. The intent of the platform is to allow people of various backgrounds, but similar interests, to connect and form communities of the like-minded that is not tied to geospatial constraints. However, the reality is that various agents create and unleash *bot* accounts that disrupt those communities and alters the digital landscape making it at times a dark and hostile place.

This project will focus on a selected set of *loaded* hashtags (meaning those whose LIWC is representative of a defined agenda) that command a respectable trending status to see whether or no it attracts these *bot* accounts and to what degree. Also to be investigated, is whether it is the human accounts or their *bot* counterparts that take the lead in establishing that trending status. I'm using as a roadmap Erin Shellman's class assignment on classifying whether a Twitter account is a bot or not, the problem statement (as well as a description of the dataset) can be found here, https://github.com/erinshellman/BI-TECH-CP303/blob/master/projects/project%202/problem_statement_project_2.md.

That project did not involve (nor describe) the gathering and ETL of the dataset presented to her students. The package I used to gather tweets to shape my dataset is as follows:

1. I leveraged another college project to first gather Twitter accounts denoted to be bots, *Debot*, found here, https://github.com/nchavoshi/debot_api. Using an API key I selected my *hashtag* and collected a set of tweets that had used the *hashtag* and were determined (by this program) to be *bot* accounts. This set included whether or no the *tweet* was a *retweet*, a *reply* or an original *tweet*. 

2. Utilising the _twitteR_ package (now deprecated and unuseable), I used the extracted account names from the group of tweets gathered from *Debot* step and gathered information on the accounts themselves, such as when the account was created and the number of tweets thus far.

3. I isolated the tweets that were original


### classification trees (as the aim is a textbook example of binary claddification); the markdown files can be found here, https://github.com/erinshellman/BI-TECH-CP303/blob/master/projects/project 2/classification-trees-in-R.Rmd, and here, https://github.com/erinshellman/BI-TECH-CP303/blob/master/projects/project%202/logistic-regression-in-R.Rmd.

The individual tweet atrributes to be looked at will include 

1. timestamp
2. retweet status
3. 

On the various accounts themselves, this project will look to the following

1. profile text
2. profile pic (is it an egg)
3. followers
### 4. following

The R packages that will be utilised in this study are as follows (with one or two being dropped, dependent on subsequent decisions made in the experimental design)

library(caret)
library(igraph)
library(httr)
library(rtweet)
library(streamR)
library(RNeo4j)
library(tidyverse)
library(stringr)
library(quanteda)
library(koRpus)

```{r, set-options, eval=FALSE, cache=FALSE}

library(twitteR)  ##package has been deprecated in deferment to _rtweet_

requestURL <- "https://api.twitter.com/oauth/request_token"
accessURL <- "https://api.twitter.com/oauth/access_token"
authURL <- "https://api.twitter.com/oauth/authorize"
consumerKey <- "dMs3guHk4y6DDdztO0JDmUTqB"
consumerSecret <- "csna8h6XWQMd3ZppMBCUuNNXDTuBAVJLNK7QphZRCd5plkES0z"
accessToken <- "1240280636-PTXARRUrytYtijYXyPWKptD9cHSCMzlAujQkATI"
accessTokenSecret <- "uo8INEJbhhe43AIXfSs5veSZ6i7IHbh8Tn1LLlYTJqOm2"
handle <- "conjja"

setup_twitter_oauth(consumerKey, consumerSecret, accessToken, accessTokenSecret)

oauth_endpoints("twitter")

myapp <- oauth_app("twitter",
key = "qkgJvUbFX70rNn1K4uq9NAuVs",
secret = "xOGEeIMUUTrCTkVwGXXwdu75kqVzEq2yzDJuZPrBwt9NfwwFJm")

twitter_token <- oauth1.0_token(oauth_endpoints, myapp)
req <- GET("https://api.twitter.com/1.1/statuses/home_timeline.json",
config(token = twitter_token))
stop_for_status(req)
content(req)


## App-only authentication https://api.twitter.com/oauth2/token
## Request token URL https://api.twitter.com/oauth/request_token
## Authorize URL https://api.twitter.com/oauth/authorize
## Access token URL https://api.twitter.com/oauth/access_token 

```

```{r, set-options, eval=FALSE, cache=FALSE}

library(reticulate)
library(XML)
#path_to_python <- "/usr/local/bin/python3.6"
path_to_python <- "/Library/Frameworks/Python.framework/Versions/3.6/bin/python3.6"
use_python(path_to_python)
dbot=import("debot")
db = dbot$DeBot('YocR3mKAc7U6hjKZrNnOCk2jjnWrqgSfwWYo8yOb')#This is my API key
cbots=htmlParse(db$get_related_bots('Charlottesville'))


#Now parse the XML data into a dataframe
botlist <- xmlToList(cbots)

library(listviewer)
flatbot <- flatten(botlist)
flatterbot <- flatten(flatbot)
remove(flatbot)
flatbot <- flatten(flatterbot)
newBotlist <- unname(sapply(flatbot, `[`, 2))

```

```{r, set-options, eval=FALSE, cache=FALSE}

my_oauth <- OAuthFactory$new(consumerKey=consumer_key,consumerSecret=consumer_secret, requestURL=requestURL, accessURL=accessURL, authURL=authURL)
my_oauth$handshake(cainfo = system.file("CurlSSL", "cacert.pem", package = "RCurl"))
save(my_oauth, file = "my_oauth.Rdata")
load("~/R/github/springboard-capstone/my_oauth.Rdata")

botAccts <- lookupUsers(newBotlist)
botAcctlist <- twListToDF(botAccts)
acctNames <- row.names.data.frame(botAcctlist)
#electionTweets <- lapply(acctNames, userStream(file.name = "", with = "user", oauth = my_oauth))

#head(botAccts)
# 
# #fromAccts <- rownames(botAcctlist)
# from <- "from:"
# fromAccts <- paste(from, rownames(botAcctlist))
# fromAccts <- gsub(" ", "", fromAccts[])

```

```{r, set-options, eval=FALSE, cache=FALSE}

library(stringr)
#head(fromAccts)

#unlistedBots <- unlist(botAcctlist[]$screenName)
#botdetails <- map(unlistedBots[], ~searchTwitteR(.x, n=100))
#botDF <- twListToDF(flatten(botdetails))
#botRT <- filter(botDF, isRetweet == "TRUE")
#botOPEN <- filter(botDF, isRetweet != "TRUE")
#acctRT <- word(botRT$text, 2)
#unique(as.character(sort(acctRT)))

recentTweetsDS <- map(unlistedBots[], ~searchTwitteR(.x, resultType = "recent", n=200))
recentTweetsDF <- twListToDF(flatten(recentTweetsDS))
botDS <- data.frame(botAcctlist$screenName, botAcctlist$id, botAcctlist$created, botAcctlist$statusesCount, langDiv = 0, mean_time_betwn_tweets = 0, bot = 0)
columnnames <- names(botDS)
columnnames[1] <- "screenName"
columnnames[2] <- "ID"
columnnames[3] <- "acct_created"
columnnames[4] <- "statusesCount"
colnames(botDS) <- columnnames

tweetsRT <- filter(recentTweetsDF, isRetweet == "TRUE")
tweetsOPEN <- filter(recentTweetsDF, isRetweet != "TRUE")
#tweeters <- unique(tweetsOPEN$screenName)
#length(tweeters)

groupedTweetsDF <- tweetsOPEN %>%
  group_by(screenName) %>%
  filter(screenName %in% botDS$screenName)
castoffDF <- setdiff(tweetsOPEN, groupedTweetsDF)

```


```{r, set-options, eval=FALSE, cache=FALSE}

#using quanteda pkg
library(quanteda)

langDiv <- textstat_lexdiv(dfm(groupedTweetsDF$text))

botDiv <- groupedTweetsDF %>%
  group_by(screenName) %>%
  summarise(meanDiv = mean(langDiv))

botDS <- left_join(botDS, botDiv)
botDS$langDiv <- botDS$meanDiv
#remove the meanDiv column for cleanup

#using koRpus package
set.kRp.env(TT.cmd="/Applications/treetagger/cmd/tree-tagger-english", lang="en")
tagged.text <- treetag("./sample.txt", debug = TRUE)
descr.tagged.text <- describe(tagged.text)[["lttr.distrib"]lex.div(tagged.text)
lex.div(measure="validation")
tagged2.text <- tokenize("./sample.txt", lang="en")
describe(tagged2.text)

##Use lapply for multiple texts
##filenames <- list.files(pattern="*.txt")
##text.tagged <- lapply(filenames, function(x) treetag(x, treetagger="manual", lang="en", TT.options=list(path=filepath, preset="en")))
##text.tagged[1]

divLang <- as.data.frame(textstat_lexdiv(dfm(CTV$text)))
rownames(divLang) <- divLang[,1]
divLang <- divLang[,-1]
divLang[mapply(is.nan, divLang)] <- NA
divLang[mapply(is.infinite, divLang)] <- NA

```

```{r, set-options, eval=FALSE, cache=FALSE}

save(access_secret, access_token, accessURL, authURL, my_oauth, requestURL, consumer_key, consumer_secret, pgmform, wkForm, wkSession, workFlow, TI_html, filled_form, filled_wkForm, txtInspect, result, pgsession, result, file = "accessObjs")
saveRDS(newBotlist, "newBotlist")
save(db, dbot, flatbot, flatterbot, acctNames, acctRT, bots, path_to_python, tweeters, unlistedBots, file = "Objets")
saveRDS(tweetsOPEN, "tweetsOPEN")
saveRDS(tweetsRT, "tweetsRT")
saveRDS(twitter, "twitter")
saveRDS(botOPEN, "botOPEN")
saveRDS(botRT, "botRT")
saveRDS(botDS, "botDS")
saveRDS(botdetails, "botdetails")
saveRDS(botAccts, "botAccts")
saveRDS(botAcctlist, "botAcctlist")
saveRDS(recentTweetsDF, "recentTweetsDF")
saveRDS(recentTweetsDS, "recentTweetsDS")
saveRDS(groupedTweetsDF, "groupedTweetsDF")
saveRDS(castoffDF, "castoffDF")
saveRDS(langDiv, "langDiv")
saveRDS(botDiv, "botDiv")

saveRDS(castoffTweetsDF, "castoffTweetsDF")
#saveRDS(selectedTweetsDF, "selectedTweetsDF")

```


##> unlist(paste(filter(tweetsOPEN, screenName == "15_margiecastro")$text, collapse = " | "))
##[1] "@mor1019chacha Kamo... | @mor1019chacha Paki tanong naman po @mor1019chacha  kung Pati ba ugali kaya niyang iparetoke,.kung Oo paretoke niya Kami# #SakangIsFine | #NewProfilePic https://t.co/bvQc2tWJZ5 | They are the most unforgettable group in KPOP WORLD. \n\n2NE1\nSNSD\nT-PARA\nKARA\nMISS A\n4MINUTE

#save.image("brooklyn.Rdata")

```{r, set-options, eval=FALSE, cache=FALSE}

library(rvest)
#code to log into webpage to submit tweets for LIWC evaluation
txtInspect <- "http://textinspector.com/account/login?redirect=/"
pgsession <-html_session(txtInspect)
pgmform <- html_form(pgsession)[[1]]
filled_form <- set_values(pgmform, 'email' = "milti@mac.com", 'password' = "rf13nn3$")
submit_form(pgsession, filled_form)

#actual form that submits the tweets
workFlow <- "http://textinspector.com/workflow"
wkSession <- html_session(workFlow)
wkForm <- html_form(wkSession)[[1]]
filled_wkForm <- set_values(wkForm, 'text' = unlist(paste(filter(tweetsOPEN, screenName == "15_margiecastro")$text, collapse = " | ")))
result <- submit_form(wkSession, filled_wkForm)

htmlParse(read_html(result$url))

```

```{r, set-options, eval=FALSE, cache=FALSE}

library(rtweet)

CTV <- search_tweets(
"#Charlottesville", n = 18000, include_rts = FALSE
)
ts_plot(CTV)

```

```{r, eval=FALSE}
#getting the source of the tweet (phone, web, other)
uniqSource <- unique(recentTweetsDF$statusSource)
inD <- which(str_detect(recentTweetsDF$statusSource, "Android"))
androidTweetsDF <- recentTweetsDF[inD,]

inD <- which(str_detect(recentTweetsDF$statusSource, "iP"))
iTweetsDF <- recentTweetsDF[inD,]

inD <- which(str_detect(recentTweetsDF$statusSource, "Web"))
webTweetsDF <- recentTweetsDF[inD,]

inD <- which(str_detect(recentTweetsDF$statusSource, paste(c("Android", "iP", "Web"), collapse = '|')))
otherTweetsDF <- recentTweetsDF[-inD,]

```

```{r}
recentTweetsDF <- readRDS("capstone dataset/recentTweetsDF")
library(qdapRegex)
AppURL <- rm_between_multiple(recentTweetsDF$statusSource,"<",">")
recentTweetsDF$App <- unlist(AppURL)
appcount <- recentTweetsDF%>%group_by(App)%>%summarize(Count=n())%>%arrange(desc(Count))
recentTweetsDF <- left_join(recentTweetsDF,appcount)
recentTweetsDF$BoN <- ifelse(recentTweetsDF$Count>4,0,1)
shredDF <- recentTweetsDF %>% select(screenName, AppURL, App, Count, BoN)
#shredDF <- recentTweetsDF %>% select(screenName, App, Count, BoN)
botDS <- left_join(botDS, shredDF, by = "screenName")
botDS <- unique(botDS)
methodCount <- botDS %>% group_by(screenName) %>% summarise(mCount=n())
botDS <- left_join(botDS, methodCount)
saveRDS(botDS, "capstone dataset/botDS")
saveRDS(shredDF, "capstone dataset/shredDF")
saveRDS(AppURL, "capstone dataset/AppURL")
saveRDS(appcount, "capstone dataset/appcount")

```

The results will be summarised in a network graph of the accounts in an attempt to detect how the community of resultant accounts are interacting and how the clustering is structured: can this be represented as a single cluster or is there any sequestering and can any interaction between the sequestered groups be found?

##steps to continue forward

- [x] Get your API
import debot db = debot.DeBot('your_api_key') db.get_related_bots('Charlottesville')
- [x] twitteR
- [x] Search for feeds
- [x] By the username of the bot
- [] Then look for hashtags in those feeds
- [] Now search twitteR again for those hashtags
- [x] This will give you a dataset which contains tweets from both bots and humans
- [] So now we have a labeled dataset
which contains both bots and humans
this is the startpoint for training a machine learning algo
Thereafter we identify a suitable classification algo



## Including Plots

You can also embed plots, for example:

```{r pressure, echo=FALSE}
plot(pressure)
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.
